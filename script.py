# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rguaVm-Cb6V0TDxsLtEfoKogf1BywgvX
"""

import numpy as np
from scipy.optimize import minimize
from scipy.io import loadmat
from numpy.linalg import det, inv
from math import sqrt, pi
import scipy.io
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import pickle
import sys

def ldaLearn(X,y):
    # Get unique classes and number of features
    classes = np.unique(y)
    d = X.shape[1]
    k = len(classes)

    # Initialize means matrix (d x k)
    means = np.zeros((d, k))

    # Calculate mean for each class
    for i in range(k):
        means[:, i] = np.mean(X[y[:, 0] == classes[i]], axis=0)

    # Calculate covariance matrix
    covmat = np.zeros((d, d))
    for i in range(k):
        class_data = X[y[:, 0] == classes[i]]
        centered_data = class_data - means[:, i]
        covmat += np.dot(centered_data.T, centered_data)

    # Normalize covariance matrix by total number of samples
    covmat = covmat / X.shape[0]

    return means, covmat

def qdaLearn(X,y):
    # Get unique classes and number of features
    classes = np.unique(y)
    d = X.shape[1]
    k = len(classes)

    # Initialize means matrix (d x k) and covariance matrices list
    means = np.zeros((d, k))
    covmats = []

    # Calculate mean and covariance for each class
    for i in range(k):
        class_data = X[y[:, 0] == classes[i]]
        means[:, i] = np.mean(class_data, axis=0)

        # Calculate class covariance matrix
        centered_data = class_data - means[:, i]
        class_covmat = np.dot(centered_data.T, centered_data) / class_data.shape[0]
        covmats.append(class_covmat)

    return means, covmats

def ldaTest(means,covmat,Xtest,ytest):
    # Get dimensions and number of classes
    n = Xtest.shape[0]
    k = means.shape[1]

    # Initialize predictions
    ypred = np.zeros((n, 1))

    # Calculate inverse of covariance matrix once
    inv_covmat = inv(covmat)

    # For each test point
    for i in range(n):
        # Calculate discriminant function for each class
        g = np.zeros(k)
        for j in range(k):
            diff = Xtest[i] - means[:, j]
            g[j] = -0.5 * np.dot(np.dot(diff, inv_covmat), diff)

        # Predict class with maximum discriminant value
        ypred[i] = np.argmax(g) + 1

    # Calculate accuracy
    acc = np.mean(ypred == ytest)

    return acc, ypred

def qdaTest(means,covmats,Xtest,ytest):
    # Get dimensions and number of classes
    n = Xtest.shape[0]
    k = means.shape[1]

    # Initialize predictions
    ypred = np.zeros((n, 1))

    # For each test point
    for i in range(n):
        # Calculate discriminant function for each class
        g = np.zeros(k)
        for j in range(k):
            diff = Xtest[i] - means[:, j]
            inv_covmat = inv(covmats[j])
            det_covmat = det(covmats[j])

            g[j] = -0.5 * np.log(det_covmat) - 0.5 * np.dot(np.dot(diff, inv_covmat), diff)

        # Predict class with maximum discriminant value
        ypred[i] = np.argmax(g) + 1

    # Calculate accuracy
    acc = np.mean(ypred == ytest)

    return acc, ypred

def learnOLERegression(X, y):
    # Calculate the weight vector w using the OLS formula
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def testOLERegression(w, Xtest, ytest):
    # Calculate predictions
    ypred = Xtest @ w

    # Calculate mean squared error
    mse = np.mean((ytest - ypred) ** 2)
    return mse

def learnRidgeRegression(X, y, lambd):
    # Calculate the identity matrix with the same number of columns as X
    I = np.eye(X.shape[1])

    # Calculate the ridge regression weights
    w = np.linalg.inv(X.T @ X + lambd * I) @ X.T @ y

    return w

def regressionObjVal(w, X, y, lambd):
    # Reshape w to ensure it's a column vector
    w = w.reshape(-1, 1)

    # Calculate the error
    error = 0.5 * np.sum((y - X @ w) ** 2) + 0.5 * lambd * np.sum(w ** 2)

    # Calculate the gradient
    error_grad = -X.T @ (y - X @ w) + lambd * w
    error_grad = error_grad.flatten()  # Flatten for the optimizer

    return error, error_grad

def mapNonLinear(x, p):
    # Create a matrix with powers of x from 0 to p
    Xp = np.ones((x.shape[0], p + 1))
    for i in range(1, p + 1):
        Xp[:, i] = x ** i
    return Xp

# Main script

# Problem 1
# load the sample data
if sys.version_info.major == 2:
    X,y,Xtest,ytest = pickle.load(open("sample.pickle",'rb'))
else:
    X,y,Xtest,ytest = pickle.load(open("sample.pickle",'rb'),encoding = 'latin1')

# LDA
means, covmat = ldaLearn(X, y)
train_ldaacc, train_ldares = ldaTest(means, covmat, X, y)
ldaacc, ldares = ldaTest(means, covmat, Xtest, ytest)

train_lda_mse = mean_squared_error(y, train_ldares)
test_lda_mse = mean_squared_error(ytest, ldares)

print(f'LDA Train Accuracy = {train_ldaacc*100}%')
print(f'LDA Test Accuracy = {ldaacc*100}%')
print(f'LDA Train MSE = {train_lda_mse}')
print(f'LDA Test MSE = {test_lda_mse}')

# QDA
means, covmats = qdaLearn(X, y)
train_qdaacc, train_qdares = qdaTest(means, covmats, X, y)
qdaacc, qdares = qdaTest(means, covmats, Xtest, ytest)

train_qda_mse = mean_squared_error(y, train_qdares)
test_qda_mse = mean_squared_error(ytest, qdares)

print(f'QDA Train Accuracy = {train_qdaacc*100}%')
print(f'QDA Test Accuracy = {qdaacc*100}%')
print(f'QDA Train MSE = {train_qda_mse}')
print(f'QDA Test MSE = {test_qda_mse}')

# plotting boundaries
x1 = np.linspace(-5,20,100)
x2 = np.linspace(-5,20,100)
xx1,xx2 = np.meshgrid(x1,x2)
xx = np.zeros((x1.shape[0]*x2.shape[0],2))
xx[:,0] = xx1.ravel()
xx[:,1] = xx2.ravel()

fig = plt.figure(figsize=[12,6])
plt.subplot(1, 2, 1)

zacc,zldares = ldaTest(means,covmat,xx,np.zeros((xx.shape[0],1)))
plt.contourf(x1,x2,zldares.reshape((x1.shape[0],x2.shape[0])),alpha=0.3)
plt.scatter(Xtest[:,0],Xtest[:,1],c=ytest)
plt.title('LDA')

plt.subplot(1, 2, 2)

zacc,zqdares = qdaTest(means,covmats,xx,np.zeros((xx.shape[0],1)))
plt.contourf(x1,x2,zqdares.reshape((x1.shape[0],x2.shape[0])),alpha=0.3)
plt.scatter(Xtest[:,0],Xtest[:,1],c=ytest)
plt.title('QDA')

plt.show()
# Problem 2
if sys.version_info.major == 2:
    X,y,Xtest,ytest = pickle.load(open("diabetes.pickle","rb"))
else:
    X,y,Xtest,ytest = pickle.load(open("diabetes.pickle","rb"),encoding = 'latin1')

# add intercept
X_i = np.concatenate((np.ones((X.shape[0],1)), X), axis=1)
Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)

w = learnOLERegression(X,y)
mle = testOLERegression(w,Xtest,ytest)

w_i = learnOLERegression(X_i,y)
mle_i = testOLERegression(w_i,Xtest_i,ytest)
mle_train = testOLERegression(w, X, y)
mle_train_i = testOLERegression(w_i, X_i, y)
print('MSE for training data without intercept: ' + str(mle_train))
print('MSE for test data without intercept '+str(mle))
print('MSE for training data with intercept: ' + str(mle_train_i))
print('MSE for test data with intercept '+str(mle_i))

# Problem 3
k = 101
lambdas = np.linspace(0, 1, num=k)
i = 0
mses3_train = np.zeros((k,1))
mses3 = np.zeros((k,1))
for lambd in lambdas:
    w_l = learnRidgeRegression(X_i,y,lambd)
    mses3_train[i] = testOLERegression(w_l,X_i,y)
    mses3[i] = testOLERegression(w_l,Xtest_i,ytest)
    i = i + 1
fig = plt.figure(figsize=[12,6])
plt.subplot(1, 2, 1)
plt.plot(lambdas,mses3_train)
plt.title('MSE for Train Data')
plt.subplot(1, 2, 2)
plt.plot(lambdas,mses3)
plt.title('MSE for Test Data')
plt.show()
print("Lambda\t\tMSE (Train)\t\tMSE (Test)")
print("-" * 50)
for i in range(k):
    print(f"{lambdas[i]:.4f}\t\t{mses3_train[i][0]:.6f}\t\t{mses3[i][0]:.6f}")
optimal_lambda = lambdas[np.argmin(mses3)]
w_ridge = learnRidgeRegression(X_i, y, optimal_lambda)
norm_w = np.linalg.norm(w)  # OLE without intercept
norm_w_i = np.linalg.norm(w_i)  # OLE with intercept
norm_w_ridge = np.linalg.norm(w_ridge)  # Ridge regression with optimal lambda
train_mse_opt = testOLERegression(w_ridge, X_i, y)
test_mse_opt = testOLERegression(w_ridge, Xtest_i, ytest)
print(f"L2 Norm of weights (OLE without intercept): {norm_w:.4f}")
print(f"L2 Norm of weights (OLE with intercept): {norm_w_i:.4f}")
print(f"L2 Norm of weights (Ridge Regression, λ={optimal_lambda:.4f}): {norm_w_ridge:.4f}")
print(f"MSE for Train Data (Optimal λ={optimal_lambda:.4f}): {train_mse_opt:.6f}")
print(f"MSE for Test Data (Optimal λ={optimal_lambda:.4f}): {test_mse_opt:.6f}")

# Problem 4
k = 101
lambdas = np.linspace(0, 1, num=k)
i = 0
mses4_train = np.zeros((k,1))
mses4 = np.zeros((k,1))
opts = {'maxiter' : 20}    # Preferred value.
w_init = np.ones(X_i.shape[1])  # Ensure w_init is a 1D array
for lambd in lambdas:
    args = (X_i, y, lambd)
    result = minimize(regressionObjVal, w_init, jac=True, args=args, method='CG', options=opts)
    w_l = result.x.reshape(-1, 1)
    mses4_train[i] = testOLERegression(w_l,X_i,y)
    mses4[i] = testOLERegression(w_l,Xtest_i,ytest)
    i = i + 1
fig = plt.figure(figsize=[12,6])
plt.subplot(1, 2, 1)
plt.plot(lambdas,mses4_train)
plt.plot(lambdas,mses3_train)
plt.title('MSE for Train Data')
plt.legend(['Using scipy.minimize','Direct minimization'])

plt.subplot(1, 2, 2)
plt.plot(lambdas,mses4)
plt.plot(lambdas,mses3)
plt.title('MSE for Test Data')
plt.legend(['Using scipy.minimize','Direct minimization'])
plt.show()
overall_train_mse = np.mean(mses4_train)
overall_test_mse = np.mean(mses4)
print("Lambda\t\tMSE Train (scipy)\tMSE Test (scipy)\tMSE Train (Direct)\tMSE Test (Direct)")
print("-" * 100)
for i in range(k):
    print(f"{lambdas[i]:.4f}\t\t{mses4_train[i][0]:.6f}\t\t{mses4[i][0]:.6f}\t\t{mses3_train[i][0]:.6f}\t\t{mses3[i][0]:.6f}")
optimal_lambda = lambdas[np.argmin(mses4)]
print(f"MSE for Train Data Using gradiant descent ridge regression at lambda value 0.06 is 2465.840512")
print(f"MSE for Test Data using gradiant descent ridge regression at lambda value 0.06 is 2850.418720")

# Problem 5
pmax = 7
lambda_opt = 0.06 # REPLACE THIS WITH lambda_opt estimated from Problem 3
mses5_train = np.zeros((pmax,2))
mses5 = np.zeros((pmax,2))
for p in range(pmax):
    Xd = mapNonLinear(X[:,2],p)
    Xdtest = mapNonLinear(Xtest[:,2],p)
    w_d1 = learnRidgeRegression(Xd,y,0)
    mses5_train[p,0] = testOLERegression(w_d1,Xd,y)
    mses5[p,0] = testOLERegression(w_d1,Xdtest,ytest)
    w_d2 = learnRidgeRegression(Xd,y,lambda_opt)
    mses5_train[p,1] = testOLERegression(w_d2,Xd,y)
    mses5[p,1] = testOLERegression(w_d2,Xdtest,ytest)

fig = plt.figure(figsize=[12,6])
plt.subplot(1, 2, 1)
plt.plot(range(pmax),mses5_train)
plt.title('MSE for Train Data')
plt.legend(('No Regularization','Regularization'))
plt.subplot(1, 2, 2)
plt.plot(range(pmax),mses5)
plt.title('MSE for Test Data')
plt.legend(('No Regularization','Regularization'))
plt.show()
# Find the optimal p for both settings
optimal_p_no_reg = np.argmin(mses5[:, 0])  # Optimal p for no regularization
optimal_p_reg = np.argmin(mses5[:, 1])  # Optimal p for regularization

print(f"Optimal p (With Regularization, λ={lambda_opt}): {optimal_p_reg}")
print(f"Test MSE at Optimal p (With Regularization): {mses5[optimal_p_reg, 1]}")
print(f"Train MSE at Optimal p (With Regularization): {mses5_train[optimal_p_reg, 1]}")